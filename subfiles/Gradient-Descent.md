---
layout: post1
title: 

---
## Gradient Descent and Cost Function

In this activity, I explored the gradient descent algorithm through the `gradient_descent_cost_function.ipynb` tutorial. 

### Experiments
- By adjusting the iteration number and learning rate, I observed how the cost decreased over time. 

### Observations
- A higher learning rate resulted in a faster decrease in cost initially, but too high a rate led to divergence.
- Increasing the iteration count allowed for more gradual convergence to the minimum cost.

### Visualizations
![Cost vs Iterations](path/to/your/cost_vs_iterations.png)
