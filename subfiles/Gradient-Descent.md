---
layout: post1
title: 

---
## Gradient Descent and Cost Function

In this activity, I explored the gradient descent algorithm through the `gradient_descent_cost_function.ipynb` tutorial. This algorithm is primarily used for optimizing machine learning models by minimizing the cost function, which quantifies the error between predicted and actual outcomes.

### Experiments
- I experimented with changing the learning rate and the number of iterations, two crucial factors that affect the gradient descent      process. I kept a close eye on how the cost function value changed over time as a result of changing these settings, since this        shows how well the algorithm found the best option.

### Observations
- #### Learning Rate Impact: 
  I found that a higher learning rate led to a more rapid decrease in the cost function initially. However, if the learning rate was    set too high, the algorithm failed to converge and instead diverged, resulting in increasing cost values. This highlighted the   delicate balance required when selecting the learning rate to ensure effective optimization.

- #### Iteration Count:
  As the number of iterations increased, the algorithm had more chances to progressively converge towards the lowest cost. I pointed    out that, although more iterations often produced better results, doing so also lengthened the computation, highlighting the   significance of striking the ideal balance between speed and precision.

